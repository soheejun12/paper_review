{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count-Based Exploration in Feature Space for Reinforcement Learning \n",
    "Jarryd Martin, Suraj Narnyanan S, Tom Enveritt, Marcus Hutter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "- agent가 불확실성을 감소할 수 있도록 효과적인 탐험 전략과 함께 강화학습 알고리즘을 사용해야 함 \n",
    "- 고차원 state-action 공간을 가진 환경에서 실현 가능한 새로운 count-based optimistic 탐험 알고리즘 소개 \n",
    "- 어떤 state와 관련되던지 불확실성을 추정할 수 있게하는 새로운 state-visit count 계산 방법 제안  \n",
    "- $\\phi-pseudocount$, 가치함수 근사화에 사용한 state space와 같은 feature represntation을 탐색하는 것을 통해 일반화 \n",
    "- $\\phi-Exploration-Bonus\\space algorithm$은  agent가 변환되지 않은 state space가 아닌 feature space에서 탐험하도록 보상 \n",
    "- 제안된 방법은 이전 방법들에 비해 비용적으로 덜 비싸고, 고차원 강화학습 환경에서 현재 동향의 최신 기술과 거의 비슷한 성적을 보임 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "#### - state space가 큰 경우 \n",
    "- agent는 학습동안 전체 space 중 작은 subset에만 방문하는 경향이 있음 \n",
    "- 일반화(generalisation)가 중요\n",
    "\n",
    "#### - exploration streatgy \n",
    "- 넓은 영역에 비해 효과적인 탐험에 대한 문제는 작은 진적을 보임 \n",
    "- $\\epsilon-greedy$ 전략\n",
    "- 유한한 MDP환경에서 더 효과적인 count-based 알고리즘들 \n",
    "    - 적은 visit-count를 가진 state에 방문하여 불확실성을 줄이도록 유도 \n",
    "    \n",
    "\n",
    "- 하지만, 이러한 알고리즘들은 고차원 state-action 공간에서 자주 비효율적 \n",
    "    - 대부분의 state는 학습동안 방문되지 않아 visit-count가 거의 모든 곳에 0 이됨 \n",
    "    \n",
    "\n",
    "- visit-count의 일반화를 통해 해결 \n",
    "    - visit-count와 더 비슷\n",
    "    - 방문하지 않은 state는 0이 되지 않음 \n",
    "    - 효율적인 방법으로 적절한 유사도 측정을 계산하는 것이 관건 \n",
    "        - 방문한 state들의 history를 모두 정렬하는 것은 사실상 불가능 \n",
    "            - 대신, state space에 대해 visit-density model를 구성하고 pseudo-count를 유도 \n",
    "            - 혹은, cluster state에 대한 locality-sensitive hashing을 사용하여 각 cluster의 등장 횟수를 세는 방법 \n",
    "            \n",
    " \n",
    "#### - In this paper, \n",
    "- 새로운 count-based 탐험 알고리즘 제안 \n",
    "    - 큰 state-action 공간에서 실현 가능 \n",
    "    - LFA(Linear Function Approximation)을 사용하는 value-based 강화학습 알고리즘과 사용 가능 \n",
    "    - state 간의 유사도 측정을 위한 visit-density 모델 구성 \n",
    "        - raw state space가 아닌 value function 근사화에 사용된 feature map을 탐색하고 그 feature map에 대하여 visit-density model 구성 \n",
    "            - model은 방문된 state들과 feature를 공유하는 state 일수록 높은 확률을 부여 \n",
    "            - 확률을 통해 일반화된 visit-count 계산 \n",
    "            - 자주 관찰된 feature는 높은 count를 가짐 \n",
    "            - 이 count는 state와 관련된 불확실성의 정도로 사용 \n",
    "            - 방문 빈도수가 적은 state에 대한 방문을 장려하기 위해 count를 통해 계산된 Exploration bonus을 사용  \n",
    "    \n",
    "    \n",
    "- 이 density model은 현존하는 알고리즘과 달리, 일반화된 visit-count 계산을 위해 state space의 차원 축소를 수행할 필요가 없음 \n",
    "     - value 추정과 불확실성 추정에 같은 저차원 feature representation을 사용 \n",
    "     - 기존 알고리즘보다 더 간단하고 비용이 덜 요구됨 \n",
    "           \n",
    "       \n",
    "- 이를 통해, 이러한 간단한 접근이 고차원 강화학습의 경우에, 최신 기술과 비슷한 성능을 보임을 시사 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Related Work \n",
    "\n",
    "### 2.2 Count-Based Exploration and Optimism \n",
    "#### - 탐험 탐색 tradeoff \n",
    "- 강화학습의 근본적인 문제 \n",
    "    - agent는 실제 전이 $P$와 보상 분포 $R$을 모르기 때문에 환경을 탐험하여 정보를 얻고 불확실성을 감소시켜야 함 \n",
    "    - 동시에 예상 축적 보상을 최대화하기 위해 현재 정보를 탐색해야함 \n",
    "    \n",
    "#### - 강력한 이론적 보장을 제공하는 많은 탐험 알고리즘은 OFU(Optimism in the face of uncertainty)면에서 구현 \n",
    "- 대부분은 tabular와 count-based \n",
    "    - state-action visit counts 테이블에서 exploration bonus를 계산 \n",
    "    - 적은 count는 높은 보너스를 얻어 덜 방문한 구역에 효과적으로 낙관적(optimistic) \n",
    "    - $\\epsilon-greedy$와 같은 random 전략보다 더 효과적 \n",
    "        - agent는 큰 보상도 받지 못하고 불확실성을 크게 감소시키지 못하는 action을 방지하게됨 \n",
    "        \n",
    " \n",
    "- 가장 대표적인 알고리즘, UCB1 bandit 알고리즘 \n",
    "    - 상위 신뢰 경계를 최대화하는 action을 선택 \n",
    "        \n",
    " \n",
    "- MDP경우에서 tabular OFU 방법 중 본 논문 방법과 가장 비슷한 것은 MBIE-EB (Model-Based Interval Esitimation with Exploration Bonuses) \n",
    "\n",
    "\n",
    "### 2.3 Exploration in Large MDPs\n",
    "#### - tabular OFU알고리즘들의 문제점 \n",
    "- 작은 MDP에서는 잘 작동하지만 큰 MDP에서는 sample complexity 문제가 커짐 \n",
    "\n",
    "#### - 최근 count-based 탐험 방법의 확장들 \n",
    "- 고차원 강화학습 문제에서 잘 작동 \n",
    "- MBIE-MB와 비슷하지만 state-action visit count를 이전에 방문한 state들과의 유사도를 정량화한 일반화된 count로 대체 \n",
    "- CTS(Context Tree Switching)density model\n",
    "    - 방문한 state와 비슷한 state일수록 높은 확률을 부여\n",
    "    - 여기서 $pseudocount$가 유도됨 \n",
    "\n",
    "\n",
    "- LSH(Localirt Sensitive Hashing) \n",
    "    - 비슷한 state들을 cluster로 묶어 count 일반화 \n",
    "    \n",
    "  \n",
    "- MBIE-EB 알고리즘 \n",
    "    - counts는 exploration bonuse로 사용됨 \n",
    "    \n",
    "    \n",
    "- 이 세 알고리즘은 random 전략보다 좋은 성능을 보임 \n",
    "    - 탐험이 어려운 큰 이산적인 영역에서 최근 선도적인 탐험 방법 \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method \n",
    "#### - agent가 불확실한 state를 방문하도록 유도하는 $\\phi-Exploration Bonus$ 알고리즘 소개 \n",
    "- 다른 optimistic count-based 탐험 알고리즘과 같이, state와 관련된 불확실성을 추정하기 위해 일반화된 state-visit count 사용 \n",
    "    - 일반화된 count는 이미 방문된 state들과 얼마나 다른가를 의미하는 새로움(novelty)의 정도 \n",
    "    - feature space에서 novelty를 계산하는 것이 더 간단하고 원칙적인 접근이며 더 효과적인 탐험이 가능함 \n",
    "    \n",
    "    \n",
    "### 3.1 A Visit-Density over Feature Space \n",
    "#### -  본 논문의 방법은 density model over feature space 를 구성 \n",
    "\n",
    "<IMG SRC=\"./FEATURE-VISIT-DENSITY-MODEL.PNG\" WIDTH=400>\n",
    "    \n",
    "- 더 자주 관찰된 state와 특징을 공유하는 state에 높은 확률을 부여 \n",
    "- 구현에 있어서는 Krichevsky-Trofomov (KT) 추정 사용 \n",
    "- feature space에서 유사도 측정을 유도 \n",
    "    - 같은 요소를 공유하는 feature vector들을 비슷하게 간주 \n",
    "    \n",
    "    - 관찰된 상태가 새로운 요소 feature들을 많이 포함할수록 그 값은 작아짐 \n",
    "    \n",
    "- feature들을 독립적으로 모델링하고 count-based 추정기들을 사용하기 때문에 본 방법은 아주 적은 데이터로부터도 근거있는 novelty 측정이 가능 \n",
    "\n",
    "\n",
    "### 3.2 The $\\phi-pseudocount$\n",
    "#### - 최근 제안된 방법들의 pseudocount에 유추하여 본 노문의 feature-visit density에 대하여 두가지 $\\phi-pseudocount$를 유도 \n",
    "\n",
    "#### 1. $naive\\space\\phi-pseudocount$\n",
    "#### 2. $\\phi-pseudocount$\n",
    "\n",
    "<img src=\"./PSEUDOCOUNT.PNG\" WIDTH=400>\n",
    "\n",
    "\n",
    "- 경험적으로, $\\phi-pseudocount$가 $naive\\space\\phi-pseudocount$보다 크며, 더 좋은 성능을 보임 \n",
    "\n",
    "\n",
    "### 3.3 Reinforcement Learning with $\\phi-EB$\n",
    "\n",
    "<IMG SRC=\"./ALGORITHM1.PNG\" WIDTH=400>\n",
    "    \n",
    "#### - $\\phi-Exploratoin\\space Bonus$\n",
    "\n",
    "<IMG SRC=\"./REWARD.PNG\" WIDTH=300>\n",
    "    \n",
    "- MBIE-EB 알고리즘에서, 이 보너스는 보상에 더해짐 \n",
    "- 본 알고리즘은 각 timestep에서 각 feature에 대해 하나씩, 최대 M개의 추정기들을 통해 업데이트됨 \n",
    "    - 본 방법의 비용(cost)는 state-action space의 크기와 상관 없음 \n",
    "    - 관찰된 feature의 개수와만 상관 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Theoretical Result \n",
    "#### - 본 논문의 pseudocount가 유사도 측정에 적절함을 증명 \n",
    "#### -  history와 적은 feature를 공유하는 state일수록 더 적은 확률을 갖고 더 적은 $\\phi-pseudocount$를 가짐 \n",
    "\n",
    "<IMG SRC=\"./COROLLARY.PNG\" WIDTH=500>\n",
    "    \n",
    "- $\\phi-pseudocount$이 직감에 의해 방문한 state의 novelty와 유사도 간의의관계를 파악할 수 있음 \n",
    "- $\\phi-pseudocount$를 최소화하는 state를 방문함을 통해, agent는 이미 방문한 state의 Hamming 유사도 경계 또한 낮출 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Empirical Evaluation \n",
    "#### -  본 논문의 평가는 다음의 연구 의문점을 답하기위해 설계 \n",
    "- features로 부터 유도된 novelty의 측정은 LFA에서 state visit-count의 일반화에 좋은 방법인가?\n",
    "- $\\phi-EB$는 모든 경우의 환경에서 또는 보상이 드문 환경에서만 향상을 보이는가?\n",
    "- $\\phi-EB$를 사용한 LFA는 최신 탐험 기법과 심층 강화학습에 견줄만한가? \n",
    "\n",
    "### 5.1 Setup \n",
    "#### - ALE(Arcade Learning Environment)의 다섯가지 게임으로 평가 \n",
    "- 최근 고차원 강화학습 환경의 표준 \n",
    "- 보상: 게임 점수로 계산 \n",
    "- 상태: 비디오 프레임 (160x210 배열, 7-bit 픽셀)\n",
    "- 액션: 18개 \n",
    "- 게임간의 탐험의 어려움 정도가 매우 다르기 때문에 특히 흥미로운 평가대상 \n",
    "\n",
    "\n",
    "- 탐험이 어려운 5개 게임 선정 \n",
    "    - sparse reward\n",
    "        - Montezuma's Revenge / Venture / Freeway \n",
    "    \n",
    "    - dense reward\n",
    "        - Frostbite / Q*bert\n",
    "        \n",
    " \n",
    "- 특히, Montezuma's Revenge와 Venture에 자원을 집중 \n",
    "    - $\\phi-EB$가 드문 보상에서 더 효과적일것이라 가정\n",
    "    - 비교할 알고리즘이 이 두 게임에 더 집중 \n",
    "    \n",
    "- ALE환경에서 LFA를 구현하기 위해 Blob-PROST feature set 사용 \n",
    " \n",
    "- 비교 baseline은 $Sarsa(\\lambda)$\n",
    "    - 본 논문 방법과 같은 feature 사용 \n",
    "    - $\\epsilon-greedy$ 전략 사용 \n",
    "    - $Sarsa-\\epsilon$ \n",
    "    \n",
    "- 모든 게임에서 $\\phi-exploration\\space bonus$의 $\\beta$는 0.05\n",
    "\n",
    "\n",
    "### 5.2  Result \n",
    "\n",
    "<IMG SRC=\"./FIGURE1.PNG\" WIDTH=800>\n",
    "    \n",
    "    \n",
    "<IMG SRC=\"./TABLE1.PNG\" WIDTH=800>\n",
    "    \n",
    "#### - 전반적으로 좋은 결과 \n",
    "#### - Freeway, 경우 \n",
    "- $\\beta = 0.05$보다 $0.035$일 때 더 좋은 결과를 보임 \n",
    "- $\\beta$에 대한 민감도는 Freeway에서 활동된 최적 정책과는 관계가 없는 feature의 크기가 컸기 때문이라 생각 \n",
    "- 그러므로, 효과적인 optimistic exploration은 task-relevant feature의 관점에서 novelty 측정을 포함해야함 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion \n",
    "#### - 고차원 환경에서의 count-based optimistic exploration 전략인 $\\phi-Exploration\\space Bonus$ 방법 소개 \n",
    "- 기존 방법들보다 구현이 간단하고 계산적 비용이 적음 \n",
    "\n",
    "#### - 좋은 성능을 보임 \n",
    "- $\\epsilon-greedy$ exploration \n",
    "- 심층 강화학습을위한 exploration 기법 \n",
    "\n",
    "#### - 탐험을위해 visit-count를 일반화할 때, take-relevant feature의 관점에서 novelty를 계산하는 것이 효과적 \n",
    "\n",
    "#### - LFA에 제한된 결과임을 주의 \n",
    "- 최근 강화학습에서 진행되고 있는 비선형함수 근사화 기법과 어떻게 결합될 수 있을지는 모름 \n",
    "- 이러한 간단한 방법의 성공이 이와 같은 방향에서 새로운 연구에 영감을 주길 바란다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
